{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 1: Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T10:50:25.930745Z",
     "start_time": "2020-10-23T10:50:25.850067Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from dp import policy_iteration, value_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T10:50:26.393138Z",
     "start_time": "2020-10-23T10:50:26.390321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Action mappings\n",
    "action_mapping = {\n",
    "    0: '\\u2190',  # LEFT\n",
    "    1: '\\u2193',  # DOWN\n",
    "    2: '\\u2192',  # RIGHT\n",
    "    3: '\\u2191',  # UP\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T10:50:26.947020Z",
     "start_time": "2020-10-23T10:50:26.942474Z"
    }
   },
   "outputs": [],
   "source": [
    "def play_episodes(environment, n_episodes, policy):\n",
    "    wins = 0\n",
    "    total_reward = 0\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "\n",
    "        terminated = False\n",
    "        state = environment.reset()\n",
    "\n",
    "        while not terminated:\n",
    "\n",
    "            # Select best action to perform in a current state\n",
    "            action = np.argmax(policy[state])\n",
    "\n",
    "            # Perform an action an observe how environment acted in response\n",
    "            next_state, reward, terminated, info = environment.step(action)\n",
    "\n",
    "            # Summarize total reward\n",
    "            total_reward += reward\n",
    "\n",
    "            # Update current state\n",
    "            state = next_state\n",
    "\n",
    "            # Calculate number of wins over episodes\n",
    "            if terminated and reward == 1.0:\n",
    "                wins += 1\n",
    "\n",
    "    average_reward = total_reward / n_episodes\n",
    "\n",
    "    return wins, total_reward, average_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: stochastic (slippery) frozen lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T10:50:29.173360Z",
     "start_time": "2020-10-23T10:50:28.921598Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load a Frozen Lake environment\n",
    "environment = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "\n",
    "The surface is described using a grid like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T10:50:30.408140Z",
     "start_time": "2020-10-23T10:50:30.405222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "environment.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "* F represents a Frozen tile, that is to say that if the agent is on a frozen tile and if he chooses to go in a certain direction, he won’t necessarily go in this direction.\n",
    "* H represents an Hole. If the agent falls in an hole, he dies and the game ends here.\n",
    "* G represents the Goal. If the agent reaches the goal, you win the game.\n",
    "* S represents the Start state. This is where the agent is at the beginning of the game.\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "More information available [here](http://gym.openai.com/envs/FrozenLake-v0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/frozenlake.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T10:50:36.802824Z",
     "start_time": "2020-10-23T10:50:32.159346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluated in 66 iterations.\n",
      "Policy evaluated in 414 iterations.\n",
      "Policy evaluated in 450 iterations.\n",
      "Evaluated 4 policies.\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      " Final policy derived using Policy Iteration:\n",
      "← ↑ ↑ ↑\n",
      "← ← ← ←\n",
      "↑ ↓ ← ←\n",
      "← → ↓ ←\n",
      "Policy Iteration :: number of wins over 10000 episodes = 7465\n",
      "Policy Iteration :: average reward over 10000 episodes = 0.7465 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of episodes to play\n",
    "n_episodes = 10000\n",
    "\n",
    "# Functions to find best policy\n",
    "solver_name = 'Policy Iteration'\n",
    "solver_func = policy_iteration\n",
    "\n",
    "# Set the seed for the environment\n",
    "environment.seed(0)\n",
    "\n",
    "# Search for an optimal policy using policy iteration\n",
    "policy, V = solver_func(environment.env)\n",
    "\n",
    "environment.render()\n",
    "print(f'\\n Final policy derived using {solver_name}:')\n",
    "print(' '.join([action_mapping[action] for action in np.argmax(policy[:4], axis=1)]))\n",
    "print(' '.join([action_mapping[action] for action in np.argmax(policy[4:8], axis=1)]))\n",
    "print(' '.join([action_mapping[action] for action in np.argmax(policy[8:12], axis=1)]))\n",
    "print(' '.join([action_mapping[action] for action in np.argmax(policy[12:], axis=1)]))\n",
    "\n",
    "# Apply best policy to the real environment\n",
    "wins, total_reward, average_reward = play_episodes(environment, n_episodes, policy)\n",
    "\n",
    "print(f'{solver_name} :: number of wins over {n_episodes} episodes = {wins}')\n",
    "print(f'{solver_name} :: average reward over {n_episodes} episodes = {average_reward} \\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before you run the cell below, make sure that you have completed the `value_iteration` function in `dp.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:29:01.006169Z",
     "start_time": "2020-10-01T14:29:00.773531Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of episodes to play\n",
    "n_episodes = 10000\n",
    "\n",
    "# Functions to find best policy\n",
    "solver_name = 'Value Iteration'\n",
    "solver_func = value_iteration\n",
    "\n",
    "# Set the seed for the environment\n",
    "environment.seed(0)\n",
    "\n",
    "# Search for an optimal policy using policy iteration\n",
    "policy, V = solver_func(environment.env)\n",
    "\n",
    "environment.render()\n",
    "print(f'\\n Final policy derived using {solver_name}:')\n",
    "print(' '.join([action_mapping[action] for action in np.argmax(policy[:4], axis=1)]))\n",
    "print(' '.join([action_mapping[action] for action in np.argmax(policy[4:8], axis=1)]))\n",
    "print(' '.join([action_mapping[action] for action in np.argmax(policy[8:12], axis=1)]))\n",
    "print(' '.join([action_mapping[action] for action in np.argmax(policy[12:], axis=1)]))\n",
    "\n",
    "# Apply best policy to the real environment\n",
    "wins, total_reward, average_reward = play_episodes(environment, n_episodes, policy)\n",
    "\n",
    "print(f'{solver_name} :: number of wins over {n_episodes} episodes = {wins}')\n",
    "print(f'{solver_name} :: average reward over {n_episodes} episodes = {average_reward} \\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: deterministic (non-slippery) frozen lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we modify the environment state-transition to be _deterministic_. That means if the agent is on a frozen tile and if he chooses to go in a certain direction, he will go in this direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T10:50:42.622121Z",
     "start_time": "2020-10-23T10:50:42.619336Z"
    }
   },
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='Deterministic-FrozenLake-v1', # name given to this new environment\n",
    "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv', # env entry point\n",
    "    kwargs={'map_name': '4x4', 'is_slippery': False} # argument passed to the env\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T10:50:43.396862Z",
     "start_time": "2020-10-23T10:50:43.393853Z"
    }
   },
   "outputs": [],
   "source": [
    "environment = gym.make('Deterministic-FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T10:50:43.886139Z",
     "start_time": "2020-10-23T10:50:43.882910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "environment.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the cell below to utilize Policy Iteration to find the optimal policy and value.\n",
    "\n",
    "Notice that the policy found is not optimal and results in agent being stuck in the same location (your loop might not terminate). \n",
    "\n",
    "Figure out what is wrong and how you can change the code to find the optimal policy and value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:29:36.007776Z",
     "start_time": "2020-10-01T14:29:35.299085Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of episodes to play\n",
    "n_episodes = 10000\n",
    "\n",
    "# Functions to find best policy\n",
    "solver_name = 'Policy Iteration'\n",
    "solver_func = policy_iteration\n",
    "\n",
    "# Set the seed for the environment\n",
    "environment.seed(0)\n",
    "\n",
    "# Search for an optimal policy using policy iteration\n",
    "policy, V = solver_func(environment)\n",
    "\n",
    "environment.render()\n",
    "print(f'\\n Final policy derived using {solver_name}:')\n",
    "print(' '.join([action_mapping[action] for action in np.argmax(policy[:4], axis=1)]))\n",
    "print(' '.join([action_mapping[action] for action in np.argmax(policy[4:8], axis=1)]))\n",
    "print(' '.join([action_mapping[action] for action in np.argmax(policy[8:12], axis=1)]))\n",
    "print(' '.join([action_mapping[action] for action in np.argmax(policy[12:], axis=1)]))\n",
    "\n",
    "# Apply best policy to the real environment\n",
    "wins, total_reward, average_reward = play_episodes(environment, n_episodes, policy)\n",
    "\n",
    "print(f'{solver_name} :: number of wins over {n_episodes} episodes = {wins}')\n",
    "print(f'{solver_name} :: average reward over {n_episodes} episodes = {average_reward} \\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the cell below to utilize Value Iteration to find the optimal policy and value (make sure that you have completed the `value_iteration` function in `dp.py`).\n",
    "\n",
    "Notice that the policy found is not optimal and results in agent being stuck in the same location (your loop might not terminate). \n",
    "\n",
    "Figure out what is wrong and how you can change the code to find the optimal policy and value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:23:41.154509Z",
     "start_time": "2020-10-01T14:23:40.524662Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of episodes to play\n",
    "n_episodes = 10000\n",
    "\n",
    "# Functions to find best policy\n",
    "solver_name = 'Value Iteration'\n",
    "solver_func = value_iteration\n",
    "\n",
    "# Set the seed for the environment\n",
    "environment.seed(0)\n",
    "\n",
    "# Search for an optimal policy using policy iteration\n",
    "policy, V = solver_func(environment)\n",
    "\n",
    "environment.render()\n",
    "print(f'\\n Final policy derived using {solver_name}:')\n",
    "print(' '.join([action_mapping[action] for action in np.argmax(policy[:4], axis=1)]))\n",
    "print(' '.join([action_mapping[action] for action in np.argmax(policy[4:8], axis=1)]))\n",
    "print(' '.join([action_mapping[action] for action in np.argmax(policy[8:12], axis=1)]))\n",
    "print(' '.join([action_mapping[action] for action in np.argmax(policy[12:], axis=1)]))\n",
    "\n",
    "# Apply best policy to the real environment\n",
    "wins, total_reward, average_reward = play_episodes(environment, n_episodes, policy)\n",
    "\n",
    "print(f'{solver_name} :: number of wins over {n_episodes} episodes = {wins}')\n",
    "print(f'{solver_name} :: average reward over {n_episodes} episodes = {average_reward} \\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SLS",
   "language": "python",
   "name": "sls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
